{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T04:04:10.572658Z",
     "start_time": "2025-05-18T04:04:10.564338Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torchvision\n",
    "from torch.nn import init\n",
    "\n",
    "from torchvision.models.resnet import BasicBlock, ResNet\n",
    "from torchvision.transforms import ToTensor"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T04:04:13.437749Z",
     "start_time": "2025-05-18T04:04:12.712492Z"
    }
   },
   "source": [
    "import io\n",
    "from torchvision import models, transforms\n",
    "import torch.utils.data as data_utils\n",
    "from PIL import Image\n",
    "import os\n",
    "import scipy.io as sio\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "def default_loader(path):\n",
    "    return Image.open(path)   "
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T04:04:15.298125Z",
     "start_time": "2025-05-18T04:04:15.281118Z"
    }
   },
   "source": [
    "from torchvision.models.resnet import BasicBlock, ResNet\n",
    "from torch.nn import init\n",
    "\n",
    "def conv(in_planes, out_planes, kernel_size=3, stride=1, dilation=1, bias=False, transposed=False):\n",
    "    if transposed:\n",
    "        layer = nn.ConvTranspose2d(in_planes, out_planes, kernel_size=kernel_size, stride=stride, padding=1, output_padding=1,\n",
    "                                   dilation=dilation, bias=bias)\n",
    "    else:\n",
    "        padding = (kernel_size + 2 * (dilation - 1)) // 2\n",
    "        layer = nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size, stride=stride, padding=padding, dilation=dilation, bias=bias)\n",
    "    if bias:\n",
    "        init.constant(layer.bias, 0)\n",
    "    return layer\n",
    "\n",
    "# Returns 2D batch normalisation layer\n",
    "def bn(planes):\n",
    "    layer = nn.BatchNorm2d(planes)\n",
    "    # Use mean 0, standard deviation 1 init\n",
    "    init.constant(layer.weight, 1)\n",
    "    init.constant(layer.bias, 0)\n",
    "    return layer\n",
    "\n",
    "\n",
    "class FeatureResNet(ResNet):\n",
    "    def __init__(self):\n",
    "        super().__init__(BasicBlock, [3, 14, 16, 3], 1000)\n",
    "        self.conv_f = conv(2,64, kernel_size=3,stride = 1)\n",
    "        self.ReLu_1 = nn.ReLU(inplace=True)\n",
    "        self.conv_pre = conv(512, 1024, stride=2, transposed=False)\n",
    "        self.bn_pre = bn(1024)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.conv_f(x)\n",
    "        #print('x1',x1.size())\n",
    "        x = self.bn1(x1)\n",
    "        #print(x.size())\n",
    "        x = self.relu(x)\n",
    "        #print(x.size())\n",
    "        x2 = self.maxpool(x)\n",
    "        #print('x2',x2.size())\n",
    "        x = self.layer1(x2)\n",
    "        #print(x2.size())\n",
    "        x3 = self.layer2(x)\n",
    "        #print('x3',x3.size())\n",
    "        x4 = self.layer3(x3)\n",
    "        #print('x4',x4.size())\n",
    "        x5 = self.layer4(x4)\n",
    "        #print('x5',x5.size())\n",
    "        x6 = self.ReLu_1(self.bn_pre(self.conv_pre(x5)))\n",
    "        #print('x6',x6.size())\n",
    "        return x1, x2, x3, x4, x5,x6\n",
    "\n",
    "\n",
    "class SegResNet(nn.Module):\n",
    "    def __init__(self, num_classes, pretrained_net):\n",
    "        super().__init__()\n",
    "        self.pretrained_net = pretrained_net\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        #self.conv3 = conv(1024,1024, stride=1, transposed=False)\n",
    "        #self.bn3 = bn(1024)\n",
    "        self.conv3_2 = conv(1024, 512, stride=1, transposed=False)\n",
    "        self.bn3_2 = bn(512)\n",
    "        self.conv4 = conv(512,512, stride=2, transposed=True)\n",
    "        self.bn4 = bn(512)\n",
    "        self.conv5 = conv(512, 256, stride=2, transposed=True)\n",
    "        self.bn5 = bn(256)\n",
    "        self.conv6 = conv(256, 128, stride=2, transposed=True)\n",
    "        self.bn6 = bn(128)\n",
    "        self.conv7 = conv(128, 64, stride=2, transposed=True)\n",
    "        self.bn7 = bn(64)\n",
    "        self.conv8 = conv(64, 64, stride=2, transposed=True)\n",
    "        self.bn8 = bn(64)\n",
    "        self.conv9 = conv(64, 32, stride=1, transposed=False)\n",
    "        self.bn9 = bn(32)\n",
    "        self.convadd = conv(32, 16, stride=1, transposed=False)\n",
    "        self.bnadd = bn(16)\n",
    "        self.conv10 = conv(16, num_classes,stride=2, kernel_size=3)\n",
    "        init.constant(self.conv10.weight, 0)  # Zero init\n",
    "\n",
    "    def forward(self, x):\n",
    "        #b,c,w,h = x.size()\n",
    "        #x = x.view(b,c,w,h)\n",
    "        x1, x2, x3, x4, x5, x6 = self.pretrained_net(x)\n",
    "        #x1 = x1.view(b,x1.size(1),x1.size(2),x1.size(3))\n",
    "        #x2 = x2.view(b,x2.size(1),x2.size(2),x2.size(3))\n",
    "        #x3 = x3.view(b,x3.size(1),x3.size(2),x3.size(3))\n",
    "        #x4 = x4.view(b,x4.size(1),x4.size(2),x4.size(3))\n",
    "        #x5 = x5.view(b,x5.size(1),x5.size(2),x5.size(3))\n",
    "\n",
    "        #x1 = torch.max(x1,1)[0]\n",
    "        #x2 = torch.max(x2,1)[0]\n",
    "        #x3 = torch.max(x3,1)[0]\n",
    "        #x4 = torch.max(x4,1)[0]\n",
    "        #x5 = torch.max(x5,1)[0]\n",
    "\n",
    "\n",
    "        #print(x5.size())\n",
    "        #print(x4.size())\n",
    "        #print(x1.size())\n",
    "        #x = self.relu(self.bn3(self.conv3(x6)))\n",
    "        x = self.relu(self.bn3_2(self.conv3_2(x6)))\n",
    "        \n",
    "        x = self.relu(self.bn4(self.conv4(x)))\n",
    "        x = self.relu(self.bn5(self.conv5(x)))\n",
    "        #print(x.size())\n",
    "        x = self.relu(self.bn6(self.conv6(x+x4 )))\n",
    "        #print(x.size())\n",
    "        x = self.relu(self.bn7(self.conv7(x+x3 )))\n",
    "        #print(x.size())\n",
    "        x = self.relu(self.bn8(self.conv8(x+x2 )))\n",
    "        #print(x.size())\n",
    "        x = self.relu(self.bn9(self.conv9(x+x1 )))\n",
    "        #print(x.size())\n",
    "        x = self.relu(self.bnadd(self.convadd(x)))\n",
    "        x = self.conv10(x)\n",
    "        return x\n"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T04:04:22.099580Z",
     "start_time": "2025-05-18T04:04:21.567767Z"
    }
   },
   "source": [
    "fnet = FeatureResNet()\n",
    "fcn = SegResNet(4,fnet)\n",
    "fcn = fcn.cuda()\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\13597\\AppData\\Local\\Temp\\ipykernel_2528\\463579634.py:19: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.\n",
      "  init.constant(layer.weight, 1)\n",
      "C:\\Users\\13597\\AppData\\Local\\Temp\\ipykernel_2528\\463579634.py:20: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.\n",
      "  init.constant(layer.bias, 0)\n",
      "C:\\Users\\13597\\AppData\\Local\\Temp\\ipykernel_2528\\463579634.py:78: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.\n",
      "  init.constant(self.conv10.weight, 0)  # Zero init\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T04:04:24.076154Z",
     "start_time": "2025-05-18T04:04:24.036598Z"
    }
   },
   "source": [
    "dataset_path = '../DIC-dataset/'\n",
    "# filename = \"validation_dataset.txt\"\n",
    "# mynumbers = []\n",
    "# with open(filename) as f:\n",
    "#     for line in f:\n",
    "#         item = line.strip().split('\\n')\n",
    "#         for subitem in item:\n",
    "#             mynumbers.append(subitem)\n",
    "            \n",
    "test_set = []\n",
    "for i in range(4000):\n",
    "    test_set.append((dataset_path+'imgs3/train_image_'+str(i+1)+'_1.png',\n",
    "                       dataset_path+'imgs3/train_image_'+str(i+1)+'_2.png',\n",
    "                       dataset_path+'gt3/train_image_'+str(i+1)+'.mat'))\n",
    "\n",
    "# dataset_path = ''\n",
    "# filename = \"train_dataset.txt\"\n",
    "# mynumbers = []\n",
    "# with open(filename) as f:\n",
    "#     for line in f:\n",
    "#         item = line.strip().split('\\n')\n",
    "#         for subitem in item:\n",
    "#             mynumbers.append(subitem)\n",
    "            \n",
    "train_set = []\n",
    "for z in range(16000):\n",
    "    train_set.append((dataset_path+'imgs3/train_image_'+str(z+1)+'_1.png',\n",
    "                       dataset_path+'imgs3/train_image_'+str(z+1)+'_2.png',\n",
    "                       dataset_path+'gt3/train_image_'+str(z+1)+'.mat'))\n",
    "    "
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T04:04:28.832247Z",
     "start_time": "2025-05-18T04:04:28.713721Z"
    }
   },
   "source": [
    "import scipy.io as sio\n",
    "from scipy import interpolate\n",
    "x = np.arange(0,256,1)\n",
    "y = np.arange(0,256,1)\n",
    "xnew = np.arange(1.5,257.5,4)\n",
    "ynew = np.arange(1.5,257.5,4)\n",
    "\n",
    "class MyDataset(data_utils.Dataset):\n",
    "    def __init__(self, dataset, transform=None, target_transform=None, loader=default_loader):\n",
    "       \n",
    "        self.imgs = dataset\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.loader = loader\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        label_x, label_y, label_z = self.imgs[index]\n",
    "        img1 = self.loader(label_x)\n",
    "        img_1 = ToTensor()(img1.resize((128,128)))\n",
    "        img2 = self.loader(label_y)\n",
    "        img_2 = ToTensor()(img2.resize((128,128)))\n",
    "        imgs = torch.cat((img_1, img_2), 0)\n",
    "        try:\n",
    "            gt = sio.loadmat(label_z)['Disp_field_1'].astype(float)\n",
    "            \n",
    "        except KeyError:\n",
    "            gt = sio.loadmat(label_z)['Disp_field_2'].astype(float)\n",
    "           \n",
    "        gt = np.asarray(gt)\n",
    "        gt = gt*100\n",
    "        [dudx, dudy]= np.gradient(gt[:,:,0])\n",
    "        [dvdx, dvdy]= np.gradient(gt[:,:,1])\n",
    "        \n",
    "        f = interpolate.interp2d(x, y, dudx, kind='cubic')\n",
    "        dudx_ = f(xnew, ynew)\n",
    "        f = interpolate.interp2d(x, y, dudy, kind='cubic')\n",
    "        dudy_ = f(xnew, ynew)\n",
    "        f = interpolate.interp2d(x, y, dvdx, kind='cubic')\n",
    "        dvdx_ = f(xnew, ynew)\n",
    "        f = interpolate.interp2d(x, y, dvdy, kind='cubic')\n",
    "        dvdy_ = f(xnew, ynew)\n",
    "        st = np.stack([dudx_, dudy_, dvdx_, dvdy_], axis=0)\n",
    "                #st = np.stack([dudx, dudy, dvdx, dvdy], axis=0)\n",
    "        \n",
    "        \n",
    "        return imgs,st\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T04:04:32.150778Z",
     "start_time": "2025-05-18T04:04:32.142773Z"
    }
   },
   "source": [
    "EPOCH = 100              # train the training data n times, to save time, we just train 100 epoch\n",
    "BATCH_SIZE = 12\n",
    "print('BATCH_SIZE = ',BATCH_SIZE)\n",
    "LR = 0.001              # learning rate\n",
    "#root = './gdrive_northwestern/My Drive/dl_encoder/data/orig/orig'\n",
    "NUM_WORKERS = 0\n",
    "\n",
    "optimizer = torch.optim.Adam(fcn.parameters(), lr=LR)   # optimize all cnn parameters\n",
    "#optimizer = torch.optim.SGD(cnn.parameters(), lr=LR, momentum=0.9)   # optimize all cnn parameters\n",
    "loss_func = nn.MSELoss()\n",
    "\n",
    "\n",
    "train_data=MyDataset(dataset=train_set)\n",
    "train_loader = data_utils.DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
    "\n",
    "test_data=MyDataset(dataset=test_set)\n",
    "test_loader = data_utils.DataLoader(dataset=test_data, batch_size=1)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BATCH_SIZE =  12\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T04:04:35.181571Z",
     "start_time": "2025-05-18T04:04:35.167564Z"
    }
   },
   "source": [
    "from datetime import datetime\n",
    "dataString = datetime.strftime(datetime.now(), '%Y_%m_%d_%H_%M_%S')"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T04:04:37.088198Z",
     "start_time": "2025-05-18T04:04:37.073879Z"
    }
   },
   "source": [
    "root_result = '../output/'\n",
    "if not os.path.exists(root_result):\n",
    "    os.mkdir(root_result)\n",
    "model_result = root_result+'model/'\n",
    "log_result = root_result+'log/'\n",
    "if not os.path.exists(model_result):\n",
    "    os.mkdir(model_result)\n",
    "\n",
    "if not os.path.exists(log_result):\n",
    "    os.mkdir(log_result)"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T04:04:40.165297Z",
     "start_time": "2025-05-18T04:04:38.511633Z"
    }
   },
   "source": [
    "fileOut=open(log_result+'log'+dataString,'a')\n",
    "fileOut.write(dataString+'Epoch:   Step:    Loss:        Val_Accu :\\n')\n",
    "fileOut.close()\n",
    "fileOut2 = open(log_result+'validation'+dataString, 'a')\n",
    "fileOut2.write('kernal_size of conv_f is 2')\n",
    "fileOut2.write(dataString+'Epoch:    loss:')\n",
    "\n",
    "optimizer = torch.optim.Adam(fcn.parameters(), lr=LR )   # optimize all cnn parameters\n",
    "\n",
    "# fcn.load_state_dict(torch.load(model_result + 'PATH_TO_PRETRAINED')) #comment this line if you start a new training\n",
    "for epoch in range(EPOCH):\n",
    "    fcn.train()\n",
    "    for step, (img,gt) in enumerate(train_loader):   # gives batch data, normalize x when iterate train_loader\n",
    "        \n",
    "        img = Variable(img).cuda()\n",
    "        gt=gt.float()\n",
    "        gt = Variable(gt).cuda()\n",
    "        output = fcn(img)               # cnn output\n",
    "        loss = loss_func(output, gt)    # loss\n",
    "        optimizer.zero_grad()           # clear gradients for this training step\n",
    "        loss.backward()                 # backpropagation, compute gradients\n",
    "        optimizer.step()                # apply gradients\n",
    "        print(epoch,  step, loss.data.item())\n",
    "        fileOut=open(log_result+'log'+dataString,'a')\n",
    "        fileOut.write(str(epoch)+'   '+str(step)+'   '+str(loss.data.item())+'\\n')\n",
    "        fileOut.close()\n",
    "    if epoch%10 == 9:\n",
    "        PATH = model_result + 'param_all_strain2_' + str(epoch) + '_' + str(step)\n",
    "        torch.save(fcn.state_dict(), PATH)\n",
    "        print('finished saving checkpoints')\n",
    "     \n",
    "    LOSS_VALIDATION = 0\n",
    "    fcn.eval()\n",
    "    with torch.no_grad():\n",
    "        for step, (img,gt) in enumerate(test_loader):\n",
    "\n",
    "            img = Variable(img).cuda()\n",
    "            gt=gt.unsqueeze(1)# batch x\n",
    "            gt = Variable(gt).cuda()\n",
    "            output = fcn(img) \n",
    "            LOSS_VALIDATION += loss_func(output, gt)\n",
    "        LOSS_VALIDATION = LOSS_VALIDATION/step\n",
    "        fileOut2 = open(log_result+'validation'+dataString, 'a')\n",
    "        fileOut2.write(str(epoch)+'   '+str(step)+'   '+str(LOSS_VALIDATION.data.item())+'\\n')\n",
    "        fileOut2.close()\n",
    "        print('validation error epoch  '+str(epoch)+':    '+str(LOSS_VALIDATION)+'\\n'+str(step))\n",
    "\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\13597\\AppData\\Local\\Temp\\ipykernel_2528\\2832534978.py:34: DeprecationWarning: `interp2d` is deprecated in SciPy 1.10 and will be removed in SciPy 1.14.0.\n",
      "\n",
      "For legacy code, nearly bug-for-bug compatible replacements are\n",
      "`RectBivariateSpline` on regular grids, and `bisplrep`/`bisplev` for\n",
      "scattered 2D data.\n",
      "\n",
      "In new code, for regular grids use `RegularGridInterpolator` instead.\n",
      "For scattered data, prefer `LinearNDInterpolator` or\n",
      "`CloughTocher2DInterpolator`.\n",
      "\n",
      "For more details see\n",
      "`https://scipy.github.io/devdocs/notebooks/interp_transition_guide.html`\n",
      "\n",
      "  f = interpolate.interp2d(x, y, dudx, kind='cubic')\n",
      "C:\\Users\\13597\\AppData\\Local\\Temp\\ipykernel_2528\\2832534978.py:35: DeprecationWarning: `interp2d` is deprecated in SciPy 1.10 and will be removed in SciPy 1.14.0.\n",
      "\n",
      "For legacy code, nearly bug-for-bug compatible replacements are\n",
      "`RectBivariateSpline` on regular grids, and `bisplrep`/`bisplev` for\n",
      "scattered 2D data.\n",
      "\n",
      "In new code, for regular grids use `RegularGridInterpolator` instead.\n",
      "For scattered data, prefer `LinearNDInterpolator` or\n",
      "`CloughTocher2DInterpolator`.\n",
      "\n",
      "For more details see\n",
      "`https://scipy.github.io/devdocs/notebooks/interp_transition_guide.html`\n",
      "\n",
      "  dudx_ = f(xnew, ynew)\n",
      "C:\\Users\\13597\\AppData\\Local\\Temp\\ipykernel_2528\\2832534978.py:36: DeprecationWarning: `interp2d` is deprecated in SciPy 1.10 and will be removed in SciPy 1.14.0.\n",
      "\n",
      "For legacy code, nearly bug-for-bug compatible replacements are\n",
      "`RectBivariateSpline` on regular grids, and `bisplrep`/`bisplev` for\n",
      "scattered 2D data.\n",
      "\n",
      "In new code, for regular grids use `RegularGridInterpolator` instead.\n",
      "For scattered data, prefer `LinearNDInterpolator` or\n",
      "`CloughTocher2DInterpolator`.\n",
      "\n",
      "For more details see\n",
      "`https://scipy.github.io/devdocs/notebooks/interp_transition_guide.html`\n",
      "\n",
      "  f = interpolate.interp2d(x, y, dudy, kind='cubic')\n",
      "C:\\Users\\13597\\AppData\\Local\\Temp\\ipykernel_2528\\2832534978.py:37: DeprecationWarning: `interp2d` is deprecated in SciPy 1.10 and will be removed in SciPy 1.14.0.\n",
      "\n",
      "For legacy code, nearly bug-for-bug compatible replacements are\n",
      "`RectBivariateSpline` on regular grids, and `bisplrep`/`bisplev` for\n",
      "scattered 2D data.\n",
      "\n",
      "In new code, for regular grids use `RegularGridInterpolator` instead.\n",
      "For scattered data, prefer `LinearNDInterpolator` or\n",
      "`CloughTocher2DInterpolator`.\n",
      "\n",
      "For more details see\n",
      "`https://scipy.github.io/devdocs/notebooks/interp_transition_guide.html`\n",
      "\n",
      "  dudy_ = f(xnew, ynew)\n",
      "C:\\Users\\13597\\AppData\\Local\\Temp\\ipykernel_2528\\2832534978.py:38: DeprecationWarning: `interp2d` is deprecated in SciPy 1.10 and will be removed in SciPy 1.14.0.\n",
      "\n",
      "For legacy code, nearly bug-for-bug compatible replacements are\n",
      "`RectBivariateSpline` on regular grids, and `bisplrep`/`bisplev` for\n",
      "scattered 2D data.\n",
      "\n",
      "In new code, for regular grids use `RegularGridInterpolator` instead.\n",
      "For scattered data, prefer `LinearNDInterpolator` or\n",
      "`CloughTocher2DInterpolator`.\n",
      "\n",
      "For more details see\n",
      "`https://scipy.github.io/devdocs/notebooks/interp_transition_guide.html`\n",
      "\n",
      "  f = interpolate.interp2d(x, y, dvdx, kind='cubic')\n",
      "C:\\Users\\13597\\AppData\\Local\\Temp\\ipykernel_2528\\2832534978.py:39: DeprecationWarning: `interp2d` is deprecated in SciPy 1.10 and will be removed in SciPy 1.14.0.\n",
      "\n",
      "For legacy code, nearly bug-for-bug compatible replacements are\n",
      "`RectBivariateSpline` on regular grids, and `bisplrep`/`bisplev` for\n",
      "scattered 2D data.\n",
      "\n",
      "In new code, for regular grids use `RegularGridInterpolator` instead.\n",
      "For scattered data, prefer `LinearNDInterpolator` or\n",
      "`CloughTocher2DInterpolator`.\n",
      "\n",
      "For more details see\n",
      "`https://scipy.github.io/devdocs/notebooks/interp_transition_guide.html`\n",
      "\n",
      "  dvdx_ = f(xnew, ynew)\n",
      "C:\\Users\\13597\\AppData\\Local\\Temp\\ipykernel_2528\\2832534978.py:40: DeprecationWarning: `interp2d` is deprecated in SciPy 1.10 and will be removed in SciPy 1.14.0.\n",
      "\n",
      "For legacy code, nearly bug-for-bug compatible replacements are\n",
      "`RectBivariateSpline` on regular grids, and `bisplrep`/`bisplev` for\n",
      "scattered 2D data.\n",
      "\n",
      "In new code, for regular grids use `RegularGridInterpolator` instead.\n",
      "For scattered data, prefer `LinearNDInterpolator` or\n",
      "`CloughTocher2DInterpolator`.\n",
      "\n",
      "For more details see\n",
      "`https://scipy.github.io/devdocs/notebooks/interp_transition_guide.html`\n",
      "\n",
      "  f = interpolate.interp2d(x, y, dvdy, kind='cubic')\n",
      "C:\\Users\\13597\\AppData\\Local\\Temp\\ipykernel_2528\\2832534978.py:41: DeprecationWarning: `interp2d` is deprecated in SciPy 1.10 and will be removed in SciPy 1.14.0.\n",
      "\n",
      "For legacy code, nearly bug-for-bug compatible replacements are\n",
      "`RectBivariateSpline` on regular grids, and `bisplrep`/`bisplev` for\n",
      "scattered 2D data.\n",
      "\n",
      "In new code, for regular grids use `RegularGridInterpolator` instead.\n",
      "For scattered data, prefer `LinearNDInterpolator` or\n",
      "`CloughTocher2DInterpolator`.\n",
      "\n",
      "For more details see\n",
      "`https://scipy.github.io/devdocs/notebooks/interp_transition_guide.html`\n",
      "\n",
      "  dvdy_ = f(xnew, ynew)\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../DIC-dataset/imgs3/train_image_23101_1.png'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[12], line 13\u001B[0m\n\u001B[0;32m     11\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(EPOCH):\n\u001B[0;32m     12\u001B[0m     fcn\u001B[38;5;241m.\u001B[39mtrain()\n\u001B[1;32m---> 13\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m step, (img,gt) \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(train_loader):   \u001B[38;5;66;03m# gives batch data, normalize x when iterate train_loader\u001B[39;00m\n\u001B[0;32m     15\u001B[0m         img \u001B[38;5;241m=\u001B[39m Variable(img)\u001B[38;5;241m.\u001B[39mcuda()\n\u001B[0;32m     16\u001B[0m         gt\u001B[38;5;241m=\u001B[39mgt\u001B[38;5;241m.\u001B[39mfloat()\n",
      "File \u001B[1;32mD:\\ProgramData\\miniconda3\\envs\\deepdic\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:634\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    631\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    632\u001B[0m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[0;32m    633\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[1;32m--> 634\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    635\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m    636\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[0;32m    637\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[0;32m    638\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called:\n",
      "File \u001B[1;32mD:\\ProgramData\\miniconda3\\envs\\deepdic\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:678\u001B[0m, in \u001B[0;36m_SingleProcessDataLoaderIter._next_data\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    676\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21m_next_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    677\u001B[0m     index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_index()  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[1;32m--> 678\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_dataset_fetcher\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfetch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[0;32m    679\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory:\n\u001B[0;32m    680\u001B[0m         data \u001B[38;5;241m=\u001B[39m _utils\u001B[38;5;241m.\u001B[39mpin_memory\u001B[38;5;241m.\u001B[39mpin_memory(data, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory_device)\n",
      "File \u001B[1;32mD:\\ProgramData\\miniconda3\\envs\\deepdic\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001B[0m, in \u001B[0;36m_MapDatasetFetcher.fetch\u001B[1;34m(self, possibly_batched_index)\u001B[0m\n\u001B[0;32m     49\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39m__getitems__(possibly_batched_index)\n\u001B[0;32m     50\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m---> 51\u001B[0m         data \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[idx] \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n\u001B[0;32m     52\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     53\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n",
      "File \u001B[1;32mD:\\ProgramData\\miniconda3\\envs\\deepdic\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001B[0m, in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m     49\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39m__getitems__(possibly_batched_index)\n\u001B[0;32m     50\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m---> 51\u001B[0m         data \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdataset\u001B[49m\u001B[43m[\u001B[49m\u001B[43midx\u001B[49m\u001B[43m]\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n\u001B[0;32m     52\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     53\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n",
      "Cell \u001B[1;32mIn[8], line 18\u001B[0m, in \u001B[0;36mMyDataset.__getitem__\u001B[1;34m(self, index)\u001B[0m\n\u001B[0;32m     16\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21m__getitem__\u001B[39m(\u001B[38;5;28mself\u001B[39m, index):\n\u001B[0;32m     17\u001B[0m     label_x, label_y, label_z \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mimgs[index]\n\u001B[1;32m---> 18\u001B[0m     img1 \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mloader\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlabel_x\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     19\u001B[0m     img_1 \u001B[38;5;241m=\u001B[39m ToTensor()(img1\u001B[38;5;241m.\u001B[39mresize((\u001B[38;5;241m128\u001B[39m,\u001B[38;5;241m128\u001B[39m)))\n\u001B[0;32m     20\u001B[0m     img2 \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mloader(label_y)\n",
      "Cell \u001B[1;32mIn[4], line 12\u001B[0m, in \u001B[0;36mdefault_loader\u001B[1;34m(path)\u001B[0m\n\u001B[0;32m     11\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mdefault_loader\u001B[39m(path):\n\u001B[1;32m---> 12\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mImage\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mopen\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\ProgramData\\miniconda3\\envs\\deepdic\\lib\\site-packages\\PIL\\Image.py:3465\u001B[0m, in \u001B[0;36mopen\u001B[1;34m(fp, mode, formats)\u001B[0m\n\u001B[0;32m   3462\u001B[0m     filename \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39mfspath(fp)\n\u001B[0;32m   3464\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m filename:\n\u001B[1;32m-> 3465\u001B[0m     fp \u001B[38;5;241m=\u001B[39m \u001B[43mbuiltins\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mopen\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilename\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mrb\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m   3466\u001B[0m     exclusive_fp \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m   3467\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: '../DIC-dataset/imgs3/train_image_23101_1.png'"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
